{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport scipy\nfrom xml.etree.ElementTree import XML, fromstring\nfrom pathlib import Path\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-09T15:46:48.423990Z","iopub.execute_input":"2023-02-09T15:46:48.424954Z","iopub.status.idle":"2023-02-09T15:46:54.314451Z","shell.execute_reply.started":"2023-02-09T15:46:48.424794Z","shell.execute_reply":"2023-02-09T15:46:54.312891Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom pathlib import Path\nimport os\nimport numpy as np\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport statistics","metadata":{"execution":{"iopub.status.busy":"2023-02-09T15:46:54.316846Z","iopub.execute_input":"2023-02-09T15:46:54.317698Z","iopub.status.idle":"2023-02-09T15:46:54.326489Z","shell.execute_reply.started":"2023-02-09T15:46:54.317648Z","shell.execute_reply":"2023-02-09T15:46:54.325197Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path_images = Path(\"/kaggle/input/face-mask-detection/images\")\npath_annot = Path(\"/kaggle/input/face-mask-detection/annotations\")","metadata":{"execution":{"iopub.status.busy":"2023-02-09T17:51:21.451574Z","iopub.execute_input":"2023-02-09T17:51:21.452570Z","iopub.status.idle":"2023-02-09T17:51:21.458644Z","shell.execute_reply.started":"2023-02-09T17:51:21.452518Z","shell.execute_reply":"2023-02-09T17:51:21.457452Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"image_paths = [\n    str(path_images/ f) for f in os.listdir(path_images)\n]\nannot_paths = [\n    str(path_annot/ f) for f in os.listdir(path_annot)\n]\n","metadata":{"execution":{"iopub.status.busy":"2023-02-09T17:51:23.012508Z","iopub.execute_input":"2023-02-09T17:51:23.012924Z","iopub.status.idle":"2023-02-09T17:51:23.033988Z","shell.execute_reply.started":"2023-02-09T17:51:23.012890Z","shell.execute_reply":"2023-02-09T17:51:23.032842Z"},"trusted":true},"execution_count":192,"outputs":[]},{"cell_type":"code","source":"image_paths.sort()\nannot_paths.sort()","metadata":{"execution":{"iopub.status.busy":"2023-02-09T17:51:24.620305Z","iopub.execute_input":"2023-02-09T17:51:24.620738Z","iopub.status.idle":"2023-02-09T17:51:24.626092Z","shell.execute_reply.started":"2023-02-09T17:51:24.620703Z","shell.execute_reply":"2023-02-09T17:51:24.624924Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"code","source":"image_size = 224\n# loop over the annotations and images, preprocess them and store in lists\ntypes = ['with_mask', 'without_mask', 'mask_weared_incorrect']\n\nsize_pattern = (16,16)\nmodel_boxes = 4\nimages = []\n\nAUTOTUNE = tf.data.AUTOTUNE\nBATCH_SIZE = 32","metadata":{"execution":{"iopub.status.busy":"2023-02-09T17:51:26.307373Z","iopub.execute_input":"2023-02-09T17:51:26.307783Z","iopub.status.idle":"2023-02-09T17:51:26.313897Z","shell.execute_reply.started":"2023-02-09T17:51:26.307749Z","shell.execute_reply":"2023-02-09T17:51:26.312559Z"},"trusted":true},"execution_count":194,"outputs":[]},{"cell_type":"markdown","source":"https://towardsdatascience.com/yolo-v3-explained-ff5b850390f\nhttps://keras.io/examples/vision/object_detection_using_vision_transformer/\nhttps://keras.io/examples/vision/xray_classification_with_tpus/","metadata":{}},{"cell_type":"code","source":"bbox = []\n\ndef populateArray(i,information):\n    classification = np.zeros(1+3+4)\n    classification[0] = 1\n    classification[1+types.index(i.find(\"name\").text)] = 1\n    classification[3+0] = int(i.find(\"bndbox\").find(\"xmin\").text)/information[\"size\"][0]\n    classification[3+1] = int(i.find(\"bndbox\").find(\"ymin\").text)/information[\"size\"][1]\n    classification[3+2] = int(i.find(\"bndbox\").find(\"xmax\").text)/information[\"size\"][0]\n    classification[3+3] = int(i.find(\"bndbox\").find(\"ymax\").text)/information[\"size\"][1]\n    return classification\n    \n\n    \ndef populateFullDetectionMatrix(targets,information):\n    size = information[\"size\"]\n    classificationImage = np.zeros((size_pattern[0]*size_pattern[1]*model_boxes,7))\n    for i in targets:\n        x = (i[3]+i[5])*size_pattern[0]/2\n        y = (i[4]+i[7])*size_pattern[1]/2\n        row = classificationImage[(int(y)*size_pattern[0]+int(x))*model_boxes]\n        offset = -1\n        for index_free in range(0,7*model_boxes,7):   \n            if(classificationImage[(int(y)*size_pattern[0]+int(x))*model_boxes+index_free][0]!=1):\n                offset = index_free\n        if(offset == -1):\n            print(f\"Error collision {len(targets)} on indexes {int(x)} and {int(y)}\")\n        for set_index in range(7): \n            classificationImage[(int(y)*size_pattern[0]+int(x))*model_boxes+offset][set_index] = i[set_index]\n    return classificationImage\n            \n\nx = defaultdict(int)\nfor i in range(0, len(annot_paths)):\n    # Access bounding box coordinates\n    filename = path_annot / annot_paths[i]\n    annot = fromstring(open(filename).read())\n    information = {\n        \"size\":(0,0),\n        \"annotations\":[]\n    }\n    targets = []\n    size = annot.find(\"size\")\n    information[\"size\"] = int(size.find(\"width\").text),int(size.find(\"height\").text)\n    \n    for i in annot.findall(\"object\"):\n        classification = populateArray(i,information)\n        targets.append(classification)\n    targets = populateFullDetectionMatrix(targets,information)\n    bbox.append(targets)\n    \nbbox = np.array(bbox)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T17:53:44.679378Z","iopub.execute_input":"2023-02-09T17:53:44.679830Z","iopub.status.idle":"2023-02-09T17:53:45.469309Z","shell.execute_reply.started":"2023-02-09T17:53:44.679779Z","shell.execute_reply":"2023-02-09T17:53:45.468172Z"},"trusted":true},"execution_count":200,"outputs":[]},{"cell_type":"code","source":"print(bbox.shape)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T17:53:47.013979Z","iopub.execute_input":"2023-02-09T17:53:47.014412Z","iopub.status.idle":"2023-02-09T17:53:47.023618Z","shell.execute_reply.started":"2023-02-09T17:53:47.014377Z","shell.execute_reply":"2023-02-09T17:53:47.022700Z"},"trusted":true},"execution_count":201,"outputs":[{"name":"stdout","text":"(853, 1024, 7)\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_img_text = tf.data.Dataset.from_tensor_slices(image_paths)\ndataset_img_bbox = tf.data.Dataset.from_tensor_slices(bbox)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T17:53:49.142772Z","iopub.execute_input":"2023-02-09T17:53:49.143736Z","iopub.status.idle":"2023-02-09T17:53:49.168676Z","shell.execute_reply.started":"2023-02-09T17:53:49.143686Z","shell.execute_reply":"2023-02-09T17:53:49.167667Z"},"trusted":true},"execution_count":202,"outputs":[]},{"cell_type":"code","source":"def map_img(img_path):\n    data = tf.io.read_file(img_path)\n    img = tf.io.decode_png(data)\n    img = tf.image.resize(img,(image_size,image_size))\n    img = tf.cast(img,tf.dtypes.bfloat16)/255\n    img.set_shape((224,224,3))\n    return img\n\ndef map_bbox(bbox):\n    bbox = tf.cast(bbox,tf.dtypes.bfloat16)\n    return bbox","metadata":{"execution":{"iopub.status.busy":"2023-02-09T17:53:50.741536Z","iopub.execute_input":"2023-02-09T17:53:50.742248Z","iopub.status.idle":"2023-02-09T17:53:50.748102Z","shell.execute_reply.started":"2023-02-09T17:53:50.742207Z","shell.execute_reply":"2023-02-09T17:53:50.747137Z"},"trusted":true},"execution_count":203,"outputs":[]},{"cell_type":"code","source":"dataset_img_img = dataset_img_text.map(map_img)\ndataset_img_bbox = dataset_img_bbox.map(map_bbox)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T17:53:52.585904Z","iopub.execute_input":"2023-02-09T17:53:52.586310Z","iopub.status.idle":"2023-02-09T17:53:52.608052Z","shell.execute_reply.started":"2023-02-09T17:53:52.586276Z","shell.execute_reply":"2023-02-09T17:53:52.606883Z"},"trusted":true},"execution_count":204,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.zip((dataset_img_img,dataset_img_bbox))","metadata":{"execution":{"iopub.status.busy":"2023-02-09T17:53:55.077396Z","iopub.execute_input":"2023-02-09T17:53:55.078449Z","iopub.status.idle":"2023-02-09T17:53:55.085576Z","shell.execute_reply.started":"2023-02-09T17:53:55.078395Z","shell.execute_reply":"2023-02-09T17:53:55.084234Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"code","source":"\n\nbackbone = tf.keras.applications.efficientnet.EfficientNetB2(\n        include_top=False, input_shape=[224, 224, 3]\n    )\n","metadata":{"execution":{"iopub.status.busy":"2023-02-09T17:34:07.939705Z","iopub.execute_input":"2023-02-09T17:34:07.940249Z","iopub.status.idle":"2023-02-09T17:34:13.595431Z","shell.execute_reply.started":"2023-02-09T17:34:07.940210Z","shell.execute_reply":"2023-02-09T17:34:13.594218Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb2_notop.h5\n31793152/31790344 [==============================] - 2s 0us/step\n31801344/31790344 [==============================] - 2s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"backbone.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_backbone():\n    \"\"\"Builds ResNet50 with pre-trained imagenet weights\"\"\"\n    backbone = tf.keras.applications.efficientnet.EfficientNetB2(\n        include_top=False, input_shape=[224, 224, 3]\n    )\n\n    d4_output, d5_output, e6_output = [\n        backbone.get_layer(layer_name).output\n        for layer_name in [\"block4d_add\", \"block5d_add\", \"block6e_add\"]\n    ]\n    return tf.keras.Model(\n        inputs=[backbone.inputs], outputs=[d4_output, d5_output, e6_output]\n    )\n","metadata":{"execution":{"iopub.status.busy":"2023-02-09T17:39:42.834525Z","iopub.execute_input":"2023-02-09T17:39:42.834970Z","iopub.status.idle":"2023-02-09T17:39:42.843358Z","shell.execute_reply.started":"2023-02-09T17:39:42.834935Z","shell.execute_reply":"2023-02-09T17:39:42.841870Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"code","source":"class FeaturePyramid(tf.keras.layers.Layer):\n    \"\"\"Builds the Feature Pyramid with the feature maps from the backbone.\n\n    Attributes:\n      num_classes: Number of classes in the dataset.\n      backbone: The backbone to build the feature pyramid from.\n        Currently supports ResNet50 only.\n    \"\"\"\n\n    def __init__(self, backbone=None, **kwargs):\n        super().__init__(name=\"FeaturePyramid\", **kwargs)\n        self.backbone = backbone if backbone else get_backbone()\n        self.conv_c3_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n        self.conv_c4_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n        self.conv_c5_1x1 = tf.keras.layers.Conv2D(256, 1, 1, \"same\")\n        self.conv_c3_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n        self.conv_c4_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n        self.conv_c5_3x3 = tf.keras.layers.Conv2D(256, 3, 1, \"same\")\n        self.conv_c6_3x3 = tf.keras.layers.Conv2D(256, 3, 2, \"same\")\n        self.conv_c7_3x3 = tf.keras.layers.Conv2D(256, 3, 2, \"same\")\n        self.upsample_2x = tf.keras.layers.UpSampling2D(2)\n\n    def call(self, images, training=False):\n        c3_output, c4_output, c5_output = self.backbone(images, training=training)\n        p3_output = self.conv_c3_1x1(c3_output)\n        p4_output = self.conv_c4_1x1(c4_output)\n        p5_output = self.conv_c5_1x1(c5_output)\n        p4_output = p4_output + self.upsample_2x(p5_output)\n        p3_output = p3_output + p4_output\n        p3_output = self.conv_c3_3x3(p3_output)\n        p4_output = self.conv_c4_3x3(p4_output)\n        p5_output = self.conv_c5_3x3(p5_output)\n        return p3_output, p4_output, p5_output\n","metadata":{"execution":{"iopub.status.busy":"2023-02-09T18:13:06.215731Z","iopub.execute_input":"2023-02-09T18:13:06.216142Z","iopub.status.idle":"2023-02-09T18:13:06.229122Z","shell.execute_reply.started":"2023-02-09T18:13:06.216108Z","shell.execute_reply":"2023-02-09T18:13:06.227711Z"},"trusted":true},"execution_count":311,"outputs":[]},{"cell_type":"code","source":"def build_head(output_filters, bias_init):\n    \"\"\"Builds the class/box predictions head.\n\n    Arguments:\n      output_filters: Number of convolution filters in the final layer.\n      bias_init: Bias Initializer for the final convolution layer.\n\n    Returns:\n      A keras sequential model representing either the classification\n        or the box regression head depending on `output_filters`.\n    \"\"\"\n    head = tf.keras.Sequential([tf.keras.Input(shape=[None, None, 256])])\n    kernel_init = tf.initializers.RandomNormal(0.0, 0.01)\n    for _ in range(4):\n        head.add(\n            tf.keras.layers.Conv2D(256, 3, padding=\"same\", kernel_initializer=kernel_init)\n        )\n        head.add(tf.keras.layers.ReLU())\n    head.add(\n        tf.keras.layers.Conv2D(\n            output_filters,\n            3,\n            1,\n            padding=\"same\",\n            kernel_initializer=kernel_init,\n            bias_initializer=bias_init,\n        )\n    )\n    return head\n","metadata":{"execution":{"iopub.status.busy":"2023-02-09T18:10:46.722354Z","iopub.execute_input":"2023-02-09T18:10:46.723029Z","iopub.status.idle":"2023-02-09T18:10:46.731685Z","shell.execute_reply.started":"2023-02-09T18:10:46.722978Z","shell.execute_reply":"2023-02-09T18:10:46.730464Z"},"trusted":true},"execution_count":302,"outputs":[]},{"cell_type":"code","source":"class RetinaNet(tf.keras.Model):\n    \"\"\"A subclassed Keras model implementing the RetinaNet architecture.\n\n    Attributes:\n      num_classes: Number of classes in the dataset.\n      backbone: The backbone to build the feature pyramid from.\n        Currently supports ResNet50 only.\n    \"\"\"\n\n    def __init__(self, num_classes, backbone=None, **kwargs):\n        super().__init__(name=\"RetinaNet\", **kwargs)\n        self.fpn = FeaturePyramid(backbone)\n        self.num_classes = num_classes\n\n        prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n        self.cls_head = build_head(num_classes, prior_probability)\n        self.box_head = build_head(4, \"zeros\")\n\n    def call(self, image, training=False):\n        features = self.fpn(image, training=training)\n        N = tf.shape(image)[0]\n        cls_outputs = []\n        box_outputs = []\n        for feature in features:\n            box_outputs.append(tf.reshape(self.box_head(feature), [N, -1, 4]))\n            cls_outputs.append(\n                tf.reshape(self.cls_head(feature), [N, -1, self.num_classes])\n            )\n        cls_outputs = tf.concat(cls_outputs, axis=1)\n        box_outputs = tf.concat(box_outputs, axis=1)\n        return tf.concat([box_outputs, cls_outputs], axis=-1)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-09T18:10:48.728468Z","iopub.execute_input":"2023-02-09T18:10:48.728885Z","iopub.status.idle":"2023-02-09T18:10:48.740506Z","shell.execute_reply.started":"2023-02-09T18:10:48.728845Z","shell.execute_reply":"2023-02-09T18:10:48.739102Z"},"trusted":true},"execution_count":303,"outputs":[]},{"cell_type":"code","source":"model  = RetinaNet(3)","metadata":{"execution":{"iopub.status.busy":"2023-02-09T18:13:09.473032Z","iopub.execute_input":"2023-02-09T18:13:09.473444Z","iopub.status.idle":"2023-02-09T18:13:12.411212Z","shell.execute_reply.started":"2023-02-09T18:13:09.473411Z","shell.execute_reply":"2023-02-09T18:13:12.409952Z"},"trusted":true},"execution_count":312,"outputs":[]},{"cell_type":"code","source":"model.build(input_shape=(None,224,224,3))","metadata":{"execution":{"iopub.status.busy":"2023-02-09T18:12:50.833884Z","iopub.execute_input":"2023-02-09T18:12:50.834347Z","iopub.status.idle":"2023-02-09T18:12:51.927508Z","shell.execute_reply.started":"2023-02-09T18:12:50.834301Z","shell.execute_reply":"2023-02-09T18:12:51.926505Z"},"trusted":true},"execution_count":309,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-02-09T18:09:59.483048Z","iopub.execute_input":"2023-02-09T18:09:59.483541Z","iopub.status.idle":"2023-02-09T18:09:59.512505Z","shell.execute_reply.started":"2023-02-09T18:09:59.483494Z","shell.execute_reply":"2023-02-09T18:09:59.511270Z"},"trusted":true},"execution_count":294,"outputs":[{"name":"stdout","text":"Model: \"RetinaNet\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nFeaturePyramid (FeaturePyram multiple                  6394157   \n_________________________________________________________________\nsequential_42 (Sequential)   (None, None, None, 3)     1777155   \n_________________________________________________________________\nsequential_43 (Sequential)   (None, None, None, 4)     1779460   \n=================================================================\nTotal params: 9,950,772\nTrainable params: 9,900,861\nNon-trainable params: 49,911\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.call(np.array([i for i in dataset_img_img.take(1)]))","metadata":{"execution":{"iopub.status.busy":"2023-02-09T18:13:13.106224Z","iopub.execute_input":"2023-02-09T18:13:13.106894Z","iopub.status.idle":"2023-02-09T18:13:13.468667Z","shell.execute_reply.started":"2023-02-09T18:13:13.106846Z","shell.execute_reply":"2023-02-09T18:13:13.467567Z"},"trusted":true},"execution_count":313,"outputs":[{"name":"stderr","text":"Cleanup called...\n","output_type":"stream"},{"execution_count":313,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1, 147, 7), dtype=float32, numpy=\narray([[[-2.23902520e-03,  7.37917516e-03,  1.20629035e-02, ...,\n         -4.59080458e+00, -4.59137678e+00, -4.58650541e+00],\n        [-9.10325442e-03,  7.54119176e-03,  6.10078406e-03, ...,\n         -4.61826038e+00, -4.58748817e+00, -4.58296919e+00],\n        [-1.10668838e-02, -4.33270913e-03, -4.82718972e-03, ...,\n         -4.61144495e+00, -4.60202074e+00, -4.58423376e+00],\n        ...,\n        [ 4.54877689e-03, -2.42707366e-03,  6.84035243e-03, ...,\n         -4.59595442e+00, -4.59896231e+00, -4.60137796e+00],\n        [ 6.99355314e-03, -3.83618753e-03,  6.36196323e-03, ...,\n         -4.59686470e+00, -4.59337091e+00, -4.60604429e+00],\n        [ 4.84350603e-04,  1.42563437e-03,  1.30592864e-02, ...,\n         -4.59278536e+00, -4.58993340e+00, -4.60114098e+00]]],\n      dtype=float32)>"},"metadata":{}}]}]}