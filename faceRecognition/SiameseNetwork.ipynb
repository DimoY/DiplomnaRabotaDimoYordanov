{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 13:36:57.729541: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-20 13:36:57.729598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dimo-81ne): /proc/driver/nvidia/version does not exist\n",
      "2022-12-20 13:36:57.730838: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "all_samples_product  = json.load(open(\"data.json\",\"r\"))[\"data\"]\n",
    "list_ds =  tf.data.Dataset.from_tensor_slices(all_samples_product)\n",
    "list_ds = list_ds.shuffle(len(list_ds), reshuffle_each_iteration=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "val_size = int(len(list_ds) * 0.2)\n",
    "train_ds = list_ds.skip(val_size)\n",
    "val_ds = list_ds.take(val_size)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "img_dims = [32,32,3]\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "def decode_img(img):\n",
    "    # Convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=img_dims[2])\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # Resize the image to the desired size\n",
    "    return tf.image.resize(img, img_dims[:2])\n",
    "\n",
    "\n",
    "def process_path(file_path):\n",
    "    # Load the raw data from the file as a string\n",
    "    img:tf.Tensor = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img\n",
    "\n",
    "def process_triplets(file_items0):\n",
    "    return process_path(file_items0[0]),process_path(file_items0[1]),process_path(file_items0[2])\n",
    "\n",
    "def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "train_ds = configure_for_performance(train_ds.map(process_triplets, num_parallel_calls=AUTOTUNE))\n",
    "val_ds = configure_for_performance(val_ds.map(process_triplets, num_parallel_calls=AUTOTUNE))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset element_spec=(TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "k = train_ds.take(1)\n",
    "print(k)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.Sequential([\n",
    "            layers.Conv2D(16, 3, activation='relu',name=\"Conv16x3\"),\n",
    "            layers.MaxPool2D()\n",
    "        ],name=\"conv1\")\n",
    "        self.conv2 = tf.keras.Sequential([\n",
    "            layers.Conv2D(16, 3, activation='relu',name=\"Conv16x3\"),\n",
    "            layers.MaxPool2D()\n",
    "        ],name=\"conv2\")\n",
    "        self.attention  = tf.keras.layers.Attention()\n",
    "        self.tail = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(512,activation=\"relu\")],name=\"tail\")\n",
    "    def call(self, imgs):\n",
    "        imgs1 = self.conv1(imgs)\n",
    "        imgs2 = self.conv2(imgs1)\n",
    "        return self.tail(self.attention([imgs2,imgs2]))\n",
    "def image_encode_creator():\n",
    "    model = Encoder()\n",
    "    return model\n",
    "\n",
    "\n",
    "Encoder = image_encode_creator()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class DistanceLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return ap_distance, an_distance\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "\n",
    "anchor_input = tf.keras.layers.Input(shape=(32, 32, 3))\n",
    "positive_input = tf.keras.layers.Input(shape=(32, 32, 3))\n",
    "negative_input = tf.keras.layers.Input(shape=(32, 32, 3))\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    Encoder(anchor_input),\n",
    "    Encoder(positive_input),\n",
    "    Encoder(negative_input),\n",
    ")\n",
    "\n",
    "siamese_network = tf.keras.Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class SiameseModel(tf.keras.Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, margin=0.2):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "datasets = []\n",
    "skiperFrog = train_ds.skip(0)\n",
    "skiper = 300\n",
    "while len(skiperFrog) !=0:\n",
    "    datasets.append(skiperFrog.take(skiper))\n",
    "    skiperFrog = skiperFrog.skip(skiper)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bazinga\n"
     ]
    }
   ],
   "source": [
    "print(\"Bazinga\")\n",
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "checkpointPath = \"training_2\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fe690177af0>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "latest = tf.train.latest_checkpoint(checkpointPath)\n",
    "siamese_model.load_weights(latest)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/300 [============================>.] - ETA: 0s - loss: 0.0266WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 13:50:23.285173: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 14\u001B[0m\n\u001B[1;32m      9\u001B[0m cp_callback \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mModelCheckpoint(filepath\u001B[38;5;241m=\u001B[39mcheckpointPath,\n\u001B[1;32m     10\u001B[0m                                                  save_weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     11\u001B[0m                                                  verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m---> 14\u001B[0m     \u001B[43msiamese_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvaluation1\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mcp_callback\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[1;32m     16\u001B[0m     siamese_model\u001B[38;5;241m.\u001B[39mfit(i, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, validation_data\u001B[38;5;241m=\u001B[39mvaluation1,callbacks\u001B[38;5;241m=\u001B[39m[cp_callback])\n",
      "File \u001B[0;32m~/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/lib/python3.10/site-packages/keras/engine/training.py:1694\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1679\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_eval_data_handler\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1680\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eval_data_handler \u001B[38;5;241m=\u001B[39m data_adapter\u001B[38;5;241m.\u001B[39mget_data_handler(\n\u001B[1;32m   1681\u001B[0m         x\u001B[38;5;241m=\u001B[39mval_x,\n\u001B[1;32m   1682\u001B[0m         y\u001B[38;5;241m=\u001B[39mval_y,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1692\u001B[0m         steps_per_execution\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution,\n\u001B[1;32m   1693\u001B[0m     )\n\u001B[0;32m-> 1694\u001B[0m val_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1695\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_x\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1696\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1697\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_sample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1698\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_batch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1699\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1700\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1701\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1702\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1703\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1704\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1705\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_use_cached_eval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1706\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1707\u001B[0m val_logs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   1708\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m name: val \u001B[38;5;28;01mfor\u001B[39;00m name, val \u001B[38;5;129;01min\u001B[39;00m val_logs\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m   1709\u001B[0m }\n\u001B[1;32m   1710\u001B[0m epoch_logs\u001B[38;5;241m.\u001B[39mupdate(val_logs)\n",
      "File \u001B[0;32m~/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/lib/python3.10/site-packages/keras/engine/training.py:2040\u001B[0m, in \u001B[0;36mModel.evaluate\u001B[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m   2036\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   2037\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m, step_num\u001B[38;5;241m=\u001B[39mstep, _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2038\u001B[0m ):\n\u001B[1;32m   2039\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_test_batch_begin(step)\n\u001B[0;32m-> 2040\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2041\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   2042\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    877\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 880\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    882\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    883\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    917\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[0;32m--> 919\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_variable_creation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    920\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001B[1;32m    921\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    922\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    132\u001B[0m   (concrete_function,\n\u001B[1;32m    133\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m--> 134\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1741\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1743\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1744\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1745\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1746\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1747\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1748\u001B[0m     args,\n\u001B[1;32m   1749\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1750\u001B[0m     executing_eagerly)\n\u001B[1;32m   1751\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    377\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 378\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    385\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    386\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    387\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    390\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    391\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m~/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 52\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     55\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#DVC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "valuation1 = val_ds.take(300)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpointPath,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "for i in datasets:\n",
    "    siamese_model.fit(i, epochs=2, validation_data=valuation1,callbacks=[cp_callback])\n",
    "for i in datasets:\n",
    "    siamese_model.fit(i, epochs=2, validation_data=valuation1,callbacks=[cp_callback])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelEncoderAtention0.02%_Margin_0.2.m5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelEncoderAtention0.02%_Margin_0.2.m5/assets\n"
     ]
    }
   ],
   "source": [
    "Encoder.save(\"modelEncoderAtention0.02%_Margin_0.2.m5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
