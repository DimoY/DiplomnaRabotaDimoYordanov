{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 12:13:37.661952: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-16 12:13:38.106300: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-16 12:13:38.106329: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-16 12:13:39.892519: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 12:13:39.892679: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-16 12:13:39.892686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "path:Path = Path(\"tinyface/Training_Set\")\n",
    "paths = [Path(path/i) for i in os.listdir(path)]\n",
    "all_samples= [[Path(i/image_path).as_posix() for image_path in os.listdir(i)] for i in paths]\n",
    "hom_many_people = len(all_samples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.00015146168095202755\n",
      "0.0003029233619040551\n",
      "0.00045438504285608264\n",
      "0.0006058467238081102\n",
      "0.0007573084047601377\n",
      "0.0009087700857121653\n",
      "0.0010602317666641929\n",
      "0.0012116934476162204\n",
      "0.0013631551285682479\n",
      "0.0015146168095202753\n",
      "0.001666078490472303\n",
      "0.0018175401714243305\n",
      "0.001969001852376358\n",
      "0.0021204635333283857\n",
      "0.002271925214280413\n",
      "0.0024233868952324407\n",
      "0.002574848576184468\n",
      "0.0027263102571364957\n",
      "0.0028777719380885234\n",
      "0.0030292336190405507\n",
      "0.0031806952999925784\n",
      "0.003332156980944606\n",
      "0.0034836186618966334\n",
      "0.003635080342848661\n",
      "0.0037865420238006884\n",
      "0.003938003704752716\n",
      "0.004089465385704744\n",
      "0.0042409270666567715\n",
      "0.004392388747608798\n",
      "0.004543850428560826\n",
      "0.004695312109512854\n",
      "0.0048467737904648815\n",
      "0.004998235471416909\n",
      "0.005149697152368936\n",
      "0.005301158833320964\n",
      "0.005452620514272991\n",
      "0.005604082195225019\n",
      "0.005755543876177047\n",
      "0.0059070055571290745\n",
      "0.006058467238081101\n",
      "0.006209928919033129\n",
      "0.006361390599985157\n",
      "0.0065128522809371845\n",
      "0.006664313961889212\n",
      "0.006815775642841239\n",
      "0.006967237323793267\n",
      "0.0071186990047452945\n",
      "0.007270160685697322\n",
      "0.00742162236664935\n",
      "0.007573084047601377\n",
      "0.007724545728553404\n",
      "0.007876007409505432\n",
      "0.00802746909045746\n",
      "0.008178930771409488\n",
      "0.008330392452361515\n",
      "0.008481854133313543\n",
      "0.00863331581426557\n",
      "0.008784777495217597\n",
      "0.008936239176169624\n",
      "0.009087700857121652\n",
      "0.00923916253807368\n",
      "0.009390624219025707\n",
      "0.009542085899977735\n",
      "0.009693547580929763\n",
      "0.00984500926188179\n",
      "0.009996470942833818\n",
      "0.010147932623785846\n",
      "0.010299394304737872\n",
      "0.0104508559856899\n",
      "0.010602317666641927\n",
      "0.010753779347593955\n",
      "0.010905241028545983\n",
      "0.01105670270949801\n",
      "0.011208164390450038\n",
      "0.011359626071402066\n",
      "0.011511087752354094\n",
      "0.011662549433306121\n",
      "0.011814011114258149\n",
      "0.011965472795210175\n",
      "0.012116934476162203\n",
      "0.01226839615711423\n",
      "0.012419857838066258\n",
      "0.012571319519018286\n",
      "0.012722781199970314\n",
      "0.012874242880922341\n",
      "0.013025704561874369\n",
      "0.013177166242826397\n",
      "0.013328627923778424\n",
      "0.01348008960473045\n",
      "0.013631551285682478\n",
      "0.013783012966634506\n",
      "0.013934474647586534\n",
      "0.014085936328538561\n",
      "0.014237398009490589\n",
      "0.014388859690442617\n",
      "0.014540321371394644\n",
      "0.014691783052346672\n",
      "0.0148432447332987\n",
      "0.014994706414250727\n",
      "0.015146168095202753\n",
      "0.015297629776154781\n",
      "0.015449091457106809\n",
      "0.015600553138058837\n",
      "0.015752014819010864\n",
      "0.01590347649996289\n",
      "0.01605493818091492\n",
      "0.016206399861866946\n",
      "0.016357861542818975\n",
      "0.016509323223771\n",
      "0.01666078490472303\n",
      "0.016812246585675057\n",
      "0.016963708266627086\n",
      "0.017115169947579112\n",
      "0.01726663162853114\n",
      "0.017418093309483167\n",
      "0.017569554990435193\n",
      "0.017721016671387223\n",
      "0.01787247835233925\n",
      "0.018023940033291278\n",
      "0.018175401714243304\n",
      "0.018326863395195334\n",
      "0.01847832507614736\n",
      "0.01862978675709939\n",
      "0.018781248438051415\n",
      "0.018932710119003444\n",
      "0.01908417179995547\n",
      "0.019235633480907496\n",
      "0.019387095161859526\n",
      "0.019538556842811552\n",
      "0.01969001852376358\n",
      "0.019841480204715607\n",
      "0.019992941885667637\n",
      "0.020144403566619663\n",
      "0.020295865247571692\n",
      "0.020447326928523718\n",
      "0.020598788609475744\n",
      "0.020750250290427773\n",
      "0.0209017119713798\n",
      "0.02105317365233183\n",
      "0.021204635333283855\n",
      "0.021356097014235884\n",
      "0.02150755869518791\n",
      "0.02165902037613994\n",
      "0.021810482057091966\n",
      "0.021961943738043995\n",
      "0.02211340541899602\n",
      "0.022264867099948047\n",
      "0.022416328780900076\n",
      "0.022567790461852102\n",
      "0.022719252142804132\n",
      "0.022870713823756158\n",
      "0.023022175504708187\n",
      "0.023173637185660213\n",
      "0.023325098866612243\n",
      "0.02347656054756427\n",
      "0.023628022228516298\n",
      "0.023779483909468324\n",
      "0.02393094559042035\n",
      "0.02408240727137238\n",
      "0.024233868952324406\n",
      "0.024385330633276435\n",
      "0.02453679231422846\n",
      "0.02468825399518049\n",
      "0.024839715676132516\n",
      "0.024991177357084546\n",
      "0.025142639038036572\n",
      "0.0252941007189886\n",
      "0.025445562399940627\n",
      "0.025597024080892653\n",
      "0.025748485761844683\n",
      "0.02589994744279671\n",
      "0.026051409123748738\n",
      "0.026202870804700764\n",
      "0.026354332485652793\n",
      "0.02650579416660482\n",
      "0.02665725584755685\n",
      "0.026808717528508875\n",
      "0.0269601792094609\n",
      "0.02711164089041293\n",
      "0.027263102571364956\n",
      "0.027414564252316986\n",
      "0.02756602593326901\n",
      "0.02771748761422104\n",
      "0.027868949295173067\n",
      "0.028020410976125096\n",
      "0.028171872657077122\n",
      "0.028323334338029152\n",
      "0.028474796018981178\n",
      "0.028626257699933204\n",
      "0.028777719380885233\n",
      "0.02892918106183726\n",
      "0.02908064274278929\n",
      "0.029232104423741315\n",
      "0.029383566104693344\n",
      "0.02953502778564537\n",
      "0.0296864894665974\n",
      "0.029837951147549426\n",
      "0.029989412828501455\n",
      "0.03014087450945348\n",
      "0.030292336190405507\n",
      "0.030443797871357536\n",
      "0.030595259552309562\n",
      "0.030746721233261592\n",
      "0.030898182914213618\n",
      "0.031049644595165647\n",
      "0.031201106276117673\n",
      "0.0313525679570697\n",
      "0.03150402963802173\n",
      "0.03165549131897376\n",
      "0.03180695299992578\n",
      "0.03195841468087781\n",
      "0.03210987636182984\n",
      "0.03226133804278187\n",
      "0.03241279972373389\n",
      "0.03256426140468592\n",
      "0.03271572308563795\n",
      "0.03286718476658998\n",
      "0.033018646447542\n",
      "0.03317010812849403\n",
      "0.03332156980944606\n",
      "0.033473031490398084\n",
      "0.03362449317135011\n",
      "0.03377595485230214\n",
      "0.03392741653325417\n",
      "0.034078878214206194\n",
      "0.034230339895158224\n",
      "0.03438180157611025\n",
      "0.03453326325706228\n",
      "0.034684724938014305\n",
      "0.034836186618966335\n",
      "0.034987648299918364\n",
      "0.03513910998087039\n",
      "0.035290571661822416\n",
      "0.035442033342774446\n",
      "0.035593495023726475\n",
      "0.0357449567046785\n",
      "0.03589641838563053\n",
      "0.036047880066582556\n",
      "0.036199341747534586\n",
      "0.03635080342848661\n",
      "0.03650226510943864\n",
      "0.03665372679039067\n",
      "0.03680518847134269\n",
      "0.03695665015229472\n",
      "0.03710811183324675\n",
      "0.03725957351419878\n",
      "0.0374110351951508\n",
      "0.03756249687610283\n",
      "0.03771395855705486\n",
      "0.03786542023800689\n",
      "0.03801688191895891\n",
      "0.03816834359991094\n",
      "0.03831980528086297\n",
      "0.03847126696181499\n",
      "0.03862272864276702\n",
      "0.03877419032371905\n",
      "0.03892565200467108\n",
      "0.039077113685623104\n",
      "0.03922857536657513\n",
      "0.03938003704752716\n",
      "0.039531498728479185\n",
      "0.039682960409431214\n",
      "0.039834422090383244\n",
      "0.03998588377133527\n",
      "0.040137345452287296\n",
      "0.040288807133239325\n",
      "0.040440268814191355\n",
      "0.040591730495143384\n",
      "0.04074319217609541\n",
      "0.040894653857047436\n",
      "0.041046115537999465\n",
      "0.04119757721895149\n",
      "0.04134903889990352\n",
      "0.04150050058085555\n",
      "0.041651962261807576\n",
      "0.0418034239427596\n",
      "0.04195488562371163\n",
      "0.04210634730466366\n",
      "0.04225780898561569\n",
      "0.04240927066656771\n",
      "0.04256073234751974\n",
      "0.04271219402847177\n",
      "0.04286365570942379\n",
      "0.04301511739037582\n",
      "0.04316657907132785\n",
      "0.04331804075227988\n",
      "0.0434695024332319\n",
      "0.04362096411418393\n",
      "0.04377242579513596\n",
      "0.04392388747608799\n",
      "0.04407534915704001\n",
      "0.04422681083799204\n",
      "0.04437827251894407\n",
      "0.044529734199896094\n",
      "0.044681195880848124\n",
      "0.04483265756180015\n",
      "0.04498411924275218\n",
      "0.045135580923704205\n",
      "0.045287042604656234\n",
      "0.045438504285608264\n",
      "0.04558996596656029\n",
      "0.045741427647512316\n",
      "0.045892889328464345\n",
      "0.046044351009416375\n",
      "0.0461958126903684\n",
      "0.04634727437132043\n",
      "0.046498736052272456\n",
      "0.046650197733224485\n",
      "0.04680165941417651\n",
      "0.04695312109512854\n",
      "0.04710458277608057\n",
      "0.047256044457032596\n",
      "0.04740750613798462\n",
      "0.04755896781893665\n",
      "0.04771042949988868\n",
      "0.0478618911808407\n",
      "0.04801335286179273\n",
      "0.04816481454274476\n",
      "0.04831627622369679\n",
      "0.04846773790464881\n",
      "0.04861919958560084\n",
      "0.04877066126655287\n",
      "0.0489221229475049\n",
      "0.04907358462845692\n",
      "0.04922504630940895\n",
      "0.04937650799036098\n",
      "0.049527969671313\n",
      "0.04967943135226503\n",
      "0.04983089303321706\n",
      "0.04998235471416909\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 34\u001B[0m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m     33\u001B[0m a \u001B[38;5;241m=\u001B[39m threading\u001B[38;5;241m.\u001B[39mThread(target\u001B[38;5;241m=\u001B[39mpermutation, args\u001B[38;5;241m=\u001B[39m(i,))\n\u001B[0;32m---> 34\u001B[0m \u001B[43ma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m thread_handles\u001B[38;5;241m.\u001B[39mappend(a)\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:940\u001B[0m, in \u001B[0;36mThread.start\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    938\u001B[0m         \u001B[38;5;28;01mdel\u001B[39;00m _limbo[\u001B[38;5;28mself\u001B[39m]\n\u001B[1;32m    939\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m--> 940\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_started\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:604\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    589\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwait\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    590\u001B[0m     \u001B[38;5;124;03m\"\"\"Block until the internal flag is true.\u001B[39;00m\n\u001B[1;32m    591\u001B[0m \n\u001B[1;32m    592\u001B[0m \u001B[38;5;124;03m    If the internal flag is true on entry, return immediately. Otherwise,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    602\u001B[0m \n\u001B[1;32m    603\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 604\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cond:\n\u001B[1;32m    605\u001B[0m         signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    606\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "all_samples_combined = []\n",
    "all_samples_product = []\n",
    "\n",
    "for i in range(len(all_samples)):\n",
    "    for j in all_samples[0:i]+all_samples[i+1:]:\n",
    "        all_samples_combined.append((all_samples[i],all_samples[i],j))\n",
    "\n",
    "random.shuffle(all_samples_combined)\n",
    "\n",
    "mutex = threading.Semaphore(1)\n",
    "bazming = True\n",
    "def permutation(i):\n",
    "    if i/len(all_samples_combined)>0.05 :\n",
    "        bazming = False\n",
    "        return\n",
    "    global all_samples_product\n",
    "    res = list(itertools.product(*(all_samples_combined[i])))\n",
    "    for item in res:\n",
    "        if item[0] == item[1]:\n",
    "            res.remove(item)\n",
    "    if i%1000 == 0 :\n",
    "        print(i/len(all_samples_combined))\n",
    "    mutex.acquire()\n",
    "    all_samples_product+= res\n",
    "    mutex.release()\n",
    "\n",
    "\n",
    "\n",
    "thread_handles = []\n",
    "for i in range(len(all_samples_combined)):\n",
    "    if bazming is not True:\n",
    "        break\n",
    "    a = threading.Thread(target=permutation, args=(i,))\n",
    "    a.start()\n",
    "    thread_handles.append(a)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for i in thread_handles:\n",
    "    i.join()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "thread_handles = []\n",
    "\n",
    "paths = []\n",
    "all_samples = []\n",
    "all_samples_combined = []\n",
    "all_samples_product = all_samples_product[0:int(4708924*2/4)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "2354462"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_samples_product)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 12:15:07.155163: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-12-16 12:15:07.155224: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dimo-81ne): /proc/driver/nvidia/version does not exist\n",
      "2022-12-16 12:15:07.157673: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_ds =  tf.data.Dataset.from_tensor_slices(all_samples_product)\n",
    "list_ds = list_ds.shuffle(len(list_ds), reshuffle_each_iteration=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "val_size = int(len(list_ds) * 0.2)\n",
    "train_ds = list_ds.skip(val_size)\n",
    "val_ds = list_ds.take(val_size)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "img_dims = [32,32,3]\n",
    "batch_size = 64\n",
    "shuffle_buffer_size = 1000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "def decode_img(img):\n",
    "    # Convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.io.decode_jpeg(img, channels=img_dims[2])\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # Resize the image to the desired size\n",
    "    return tf.image.resize(img, img_dims[:2])\n",
    "\n",
    "\n",
    "def process_path(file_path):\n",
    "    # Load the raw data from the file as a string\n",
    "    img:tf.Tensor = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img\n",
    "\n",
    "def process_triplets(file_items0):\n",
    "    return process_path(file_items0[0]),process_path(file_items0[1]),process_path(file_items0[2])\n",
    "\n",
    "def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "train_ds = configure_for_performance(train_ds.map(process_triplets, num_parallel_calls=AUTOTUNE))\n",
    "val_ds = configure_for_performance(val_ds.map(process_triplets, num_parallel_calls=AUTOTUNE))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset element_spec=(TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "k = train_ds.take(1)\n",
    "print(k)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "def image_encode_creator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(8, 3,  activation='relu'),\n",
    "        layers.MaxPool2D(),\n",
    "        layers.Conv2D(8,  3, activation='relu'),\n",
    "        layers.MaxPool2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256,activation=\"relu\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "Encoder = image_encode_creator()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class DistanceLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return ap_distance, an_distance\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\n",
    "anchor_input = tf.keras.layers.Input(shape=(32, 32, 3))\n",
    "positive_input = tf.keras.layers.Input(shape=(32, 32, 3))\n",
    "negative_input = tf.keras.layers.Input(shape=(32, 32, 3))\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    Encoder(anchor_input),\n",
    "    Encoder(positive_input),\n",
    "    Encoder(negative_input),\n",
    ")\n",
    "\n",
    "siamese_network = tf.keras.Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class SiameseModel(tf.keras.Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "\n",
    "        # Let's update and return the loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        # Computing the Triplet Loss by subtracting both distances and\n",
    "        # making sure we don't get a negative value.\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 2145/29431 [=>............................] - ETA: 30:48 - loss: 0.1516"
     ]
    }
   ],
   "source": [
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer=tf.keras.optimizers.Adam(0.0001))\n",
    "siamese_model.fit(train_ds, epochs=10, validation_data=val_ds)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
